{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From corpora: download \"Brown\", from models download \"tagsets\", \"averaged_perceptron_tagger\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download() # corpora, brown, ptb, models tagsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built in pos_tag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('boy', 'NN'), ('ate', 'VB'), ('the', 'DT'), ('delicious', 'JJ'), ('cake', 'NN')]\n",
      "DT: determiner/pronoun, singular\n",
      "    this each another that 'nother\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag,word_tokenize, help\n",
    "# 1)a  --> Must tokenize string first\n",
    "\n",
    "print(pos_tag(word_tokenize('The boy ate the delicious cake')))\n",
    "\n",
    "# 1)b\n",
    "help.brown_tagset('DT') # Gets the description of a tag and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag first 10 files in simple-wki and get the Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'religion', 'activity', 'arrow', 'brothers', 'Ages', 'boat', 'hooks', 'American', 'conservationÂ', 'Carolina', 'forms', 'word', 'Union', 'leaders', 'Thomas', 'street', 'representatives', 'States', 'world', 'end', 'statistics', 'Lincoln', 'km', 'characters', 'motors', 'Tale', 'Ricky', 'reason', 'Giovanni', 'Sumter', 'Sunfish', 'Citybus', 'months', 'nets', 'princes', 'forts', 'mammals', 'start', 'victims', 'contests', 'Confederacy', 'samples', 'disease', 'birthstone', 'Spain', 'Americans', 'Protestant', 'Arkansas', 'travels', 'Drift', 'actions', 'War', 'tuna', 'laws', 'Van', 'kinds', 'flea', 'ways', 'wires', 'includeÂ', 'spin', 'net', 'Treaty', '%', 'cost', 'half', 'frogsÂ', 'worldwide', 'Flanimals', 'Aquarium', 'others', 'Catholics', 'company', 'engine', 'US', 'river', 'Muskellunge', 'spots', 'country', 'month', 'andÂ', 'bait', 'engines', 'whales', 'winter', 'U.S.A.', 'gun', 'Lucie', 'Trout', 'seals', 'study', 'Middle', 'Sea', 'Steen', 'control', 'transport', 'Civil', 'Lower', 'Darnay', 'people', 'scarce', 'add', 'slavery', 'expert', 'bacterium', 'bacteria', 'clams', 'city', 'fear', 'part', 'collection', 'independence', 'diesel', 'mouth', 'adults', 'List', 'Kaffa', 'hearts', 'society', 'United', 'trams', 'Years', 'women', 'Confederate', 'Symptoms', 'companies', 'machines', 'OsnabrÃ¼ck', 'meaning', 'dolphins', 'Fort', 'history', 'U.S.', 'production', 'rulers', 'Poets', 'shore', 'mothers', 'secession', 'VanHool', 'Jews', 'Marlin', 'Norway', 'ocean', 'Rob', 'CSA', 'cabinet', 'process', 'Night', 'rules', 'scourge', 'FBI', 'Perch', 'January', 'Swordfish', 'fisheries', 'May', 'war', 'cephalopods', 'lungs', 'Republic', 'observation', 'Easter', 'animals', 'Songkran', 'Hool', 'Africa', 'Sweet', 'industry', 'money', 'past', 'materials', 'Georgia', 'edition', 'millions', 'Walpurgis', 'peace', 'Davis', 'terror', 'sports', 'turtles', 'plague', 'asÂ', 'Black', 'bycatch', 'advisors', 'coachbuses', 'Constitution', 'crustaceans', 'wives', 'fish', 'enslavement', 'whaling', 'lots', 'Empire', 'president', 'uncles', 'trust', 'nation', 'conditions', 'Hemisphere', 'men', 'food', 'April', 'protect', 'bus', 'use', 'Ferdinand', 'goddess', 'Games', 'extinction', 'refers', 'March', 'Bubonic', 'Montgomery', 'spring', 'fever', 'gathering', 'Manette', 'Roman', 'wire', 'office', 'tracks', 'invertebrates', 'buses', 'communities', 'name', 'aperire', 'sea', 'types', 'aÂ', 'Charles', 'flowers', 'pestis', 'ships', 'flows', 'Death', 'maricultureÂ', 'Christianity', 'Rules', 'aquaculture', 'Stephens', 'Incubation', 'Carton', 'organism', 'Muslims', 'cases', 'innocence', 'addition', 'kilograms', 'Europe', 'lake', 'Bernard', 'Dickens', 'July', 'form', 'America', 'animal', 'Orthodox', 'shrimp', 'farmers', 'Pea', 'problem', 'October', 'consumption', 'Low-Floor-Bus', 'Fishing', 'Revolution', 'capita', 'job', 'wheels', 'fathers', 'time', 'Saxony', 'day', 'husbands', 'Southeast', 'example', 'Koningshooikt', 'Antwerp', 'hemisphere', 'fishermen', 'Butler', 'year', 'boats', 'Protestants', 'England', 'Holy', 'places', 'III', 'epidemic', 'Sunday', 'showers', 'Peace', 'coughing', 'bow', 'sisters', 'accord', 'adjustments', 'trolley', 'tens', 'Anglers', 'book', 'Sepsis', 'Pigs', 'molluscs', 'week', 'equivalent', 'waterÂ', 'Alabama', 'term', 'Tuna', 'Samples', 'line', 'miles', 'Habsburg', 'lands', 'Fisheries', 'range', 'paranoia', 'summer', 'Fish', 'Texas', 'farming', 'Gervais', 'catch', 'Bass', 'Reconstruction', 'Dr.', 'practice', 'Rhine-Westphalia', 'Alexander', 'constitution', 'got', 'capital', 'century', 'troubles', 'Aphrodite', 'hours', 'Dixie', 'rodÂ', 'night', 'gas', 'Pike', 'North', 'crimes', 'Florida', 'water', 'November', 'interest', 'battles', 'electricity', 'transmission', 'coach', 'damage', 'days', 'countries', 'Greek', 'tram', 'creatures', 'June', 'return', 'cod', 'Catholic', 'Sport', 'years', 'Sydney', 'pair', 'parts', 'Emperor', 'France', 'squid', 'Abraham', 'headquarters', 'rats', 'World', 'case', 'times', 'transition', 'poles', 'background', 'life', 'sport', 'Dutch', 'negotiations', 'C.', 'Virginia', 'Northern', 'fun', 'Central', 'research', 'cities', 'Latin', 'European', 'Asia', 'system', 'type', 'calledÂ', 'export', 'MÃ¼nster', 'A', 'children', 'FAO', 'method', 'skin', 'Richmond', 'Charleston', 'number', 'Calendar', 'catastrophes', 'blood', 'populationÂ', 'fishing', 'government', 'crab', 'Yersinia', 'Southern', 'Sweden', 'incubation', 'Westphalia', 'Techniques', 'rights', 'treaties', 'Mississippi', 'Louisiana', 'fisherman', 'employment', 'states', 'close', 'story', 'period', 'locations', 'antibodies', 'competition', 'hand', 'lobster', 'Eighty', 'struck', 'marlin', 'levels', 'lot', 'species', 'trolleybus', 'diamond', 'writer', 'Salmon', 'kind', 'Tennessee', 'Daisy', 'manyÂ', 'theme', 'farms', 'echinoderms', 'Carp', 'Trolleybus', 'February', 'contest', 'fleas', 'trolleybuses', 'Cities', 'theory', 'trackless', 'Thirty', 'September', 'power', 'BassÂ', 'weeks', 'December', 'warfare', 'love', 'clots', 'cultures', 'Â', 'roof', 'nephews', 'Jefferson', 'Vice-President', 'festivals', 'fuels', 'lines', 'South', 'saltwater', 'Pyrenees', 'forces', 'Boccaccio'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk import FreqDist\n",
    "from collections import defaultdict\n",
    "\n",
    "tagged = []\n",
    "\n",
    "for root, dirs, files in os.walk('C:/Users/hadeel.mostafa/Desktop/my nlp/NLP w18/Labs/datasets/simple-wiki/single-docs'):\n",
    "    for file in files[:10]:\n",
    "        with open(os.path.join(root,file)) as f:\n",
    "            tags = pos_tag(word_tokenize(f.read()))\n",
    "            tagged+=tags # Build a list of pairs of all words and their tags in first 10 files\n",
    "\n",
    "print(set([word for (word, tag) in tagged if tag[:2] == 'NN'])) # Print all words whose tags start with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a default dict, no need to initialize key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "test_dict = {}\n",
    "# test_dict['plays']+=1 # This gives an error when uncommented, key has not been intialized\n",
    "\n",
    "if 'plays' not in test_dict: # Using normal dict, must initalize key\n",
    "    test_dict['plays'] = 0\n",
    "    \n",
    "#######################################\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "test_dict = defaultdict(int)\n",
    "test_dict['plays']+=1\n",
    "\n",
    "print(test_dict['plays'])\n",
    "print(test_dict['eats'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of FreqDist objects, to count the parts of speech assigned to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VB', 10), ('NN', 3)]\n",
      "('VB', 10)\n",
      "VB\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "tags = defaultdict(lambda: FreqDist())\n",
    "\n",
    "tags['play']['NN']+=3\n",
    "tags['play']['VB']+=10\n",
    "tags['play']['DT']+=1\n",
    "\n",
    "print(tags['play'].most_common(2)) # Gets the 2 most common elements\n",
    "print(tags['play'].most_common(2)[0]) # Gets the first most common element, pair of format (tag,count)\n",
    "print(tags['play'].most_common(2)[0][0]) # Gets the tag of the first most common element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 5-3   &nbsp;&nbsp; (Unigram Tagger Implementation)\n",
    "### Loading the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]]\n",
      "Number of sents in brown corpus 57340\n",
      "Number of tokens in brown corpus 1161192\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# 3)a\n",
    "all_tagged = brown.tagged_sents()\n",
    "print(all_tagged[0:2]) # The first two sentences in the tagged corpus: List of a list of (word, tag) pairs\n",
    "print('Number of sents in brown corpus', len(all_tagged))\n",
    "print('Number of tokens in brown corpus', sum([len(sent) for sent in all_tagged]))\n",
    "\n",
    "# 3)b\n",
    "train = all_tagged[:50000] # First 50000 sentences for training\n",
    "test = all_tagged[50000:] # Rest for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the unigram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('uproar', 'NN'), ('borrowed', 'VBN'), ('potentiometer', 'NN'), ('Stetson', 'NP'), ('savory', 'JJ'), ('convalescence', 'NN'), ('barn', 'NN'), (\"'till\", 'IN'), ('demoralized', 'VBN'), ('Neglected', 'VBN-TL')]\n"
     ]
    }
   ],
   "source": [
    "# Example format for dictionary named \"words\"\n",
    "#{'play': {'VB':200, 'NN':100}, eat:{'VB':100}}\n",
    "\n",
    "# 3)c\n",
    "words = defaultdict(lambda: FreqDist())\n",
    "\n",
    "for sent in train: # Loop over all training sents\n",
    "    for (word, pos) in sent: # Loop over all words and their true parts of speech in each setence\n",
    "        words[word][pos]+=1 # This word has been tagged by this pos once\n",
    "\n",
    "# Dictionary of words (keys) and their most repeated part of speech (values)\n",
    "# 3) d\n",
    "uni = {}\n",
    "\n",
    "for word in words:\n",
    "    uni[word] = words[word].most_common(1)[0][0]\n",
    "    \n",
    "print(list(uni.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the accuracy over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model accuracy: 0.8831634672471799\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for sent in test:\n",
    "    for (word, pos) in sent:\n",
    "        # If word in test sentence is present in unigram dictionary\n",
    "        # AND if the predicted part of speech (uni[word]) is equal to the true part of speech\n",
    "        # Increment the number of correct predictions by 1\n",
    "        if word in uni and uni[word] == pos:\n",
    "            correct +=1\n",
    "        total+=1 # Count the total number of words in the test set\n",
    "        \n",
    "print('Unigram model accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## Exercise 5-4   &nbsp;&nbsp; (NLTK Taggers)\n",
    "### Build a DeafultTagger that predicts all parts of speech as nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1091925588759153\n",
      "[('The', 'NN'), ('boy', 'NN'), ('ate', 'NN'), ('the', 'NN'), ('apple', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import DefaultTagger\n",
    "\n",
    "default_tagger = DefaultTagger('NN')\n",
    "print(default_tagger.evaluate(test))\n",
    "\n",
    "print(default_tagger.tag(word_tokenize('The boy ate the apple')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8831634672471799\n",
      "[('He', 'PPS'), ('watched', 'VBD'), ('the', 'AT'), ('play', 'VB')]\n",
      "[('The', 'AT'), ('kids', 'NNS'), ('play', 'VB'), ('in', 'IN'), ('the', 'AT'), ('garden', 'NN')]\n",
      "[('I', 'PPSS'), ('saw', 'VBD'), ('a', 'AT'), ('green', 'JJ'), ('spider', 'NN')]\n",
      "[('Salah', None), ('scored', 'VBD'), ('the', 'AT'), ('goal', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import UnigramTagger\n",
    "\n",
    "unigram_tagger = UnigramTagger(train)\n",
    "\n",
    "print(unigram_tagger.evaluate(test))\n",
    "\n",
    "print(unigram_tagger.tag(word_tokenize('He watched the play'))) # Tagged play as verb\n",
    "print(unigram_tagger.tag(word_tokenize('The kids play in the garden')))\n",
    "print(unigram_tagger.tag(word_tokenize('I saw a green spider')))\n",
    "print(unigram_tagger.tag(word_tokenize('Salah scored the goal'))) # Can't find salah in train sents, tagged as none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3462464542515997\n",
      "[('He', 'PPS'), ('watched', 'VBD'), ('the', 'AT'), ('play', 'NN')]\n",
      "[('The', 'AT'), ('kids', 'NNS'), ('play', 'VB'), ('in', 'IN'), ('the', 'AT'), ('garden', 'NN')]\n",
      "[('I', 'PPSS'), ('saw', 'VBD'), ('a', 'AT'), ('green', 'JJ'), ('spider', None)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import BigramTagger\n",
    "\n",
    "bigram_tagger = BigramTagger(train)\n",
    "print(bigram_tagger.evaluate(test))\n",
    "\n",
    "print(bigram_tagger.tag(word_tokenize('He watched the play'))) # play correctly tagged as noun\n",
    "print(bigram_tagger.tag(word_tokenize('The kids play in the garden')))\n",
    "print(bigram_tagger.tag(word_tokenize('I saw a green spider'))) # can't find the bigram green spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backoff tagger, from Unigram to DefaultTagegr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8897437166039976\n",
      "[('Salah', 'NN'), ('scored', 'VBD'), ('the', 'AT'), ('goal', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "backoff_unigram_tagger = UnigramTagger(train, backoff=default_tagger)\n",
    "print(backoff_unigram_tagger.evaluate(test))\n",
    "# couldn't find salah in unigram, so backed off to noun\n",
    "print(backoff_unigram_tagger.tag(word_tokenize('Salah scored the goal'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Bigram to Unigram (which backs off to DefaultTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911174879609473\n",
      "[('I', 'PPSS'), ('saw', 'VBD'), ('a', 'AT'), ('green', 'JJ'), ('spider', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "backoff_bigram_tagger = BigramTagger(train, backoff = backoff_unigram_tagger)\n",
    "print(backoff_bigram_tagger.evaluate(test))\n",
    "print(backoff_bigram_tagger.tag(word_tokenize('I saw a green spider')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
