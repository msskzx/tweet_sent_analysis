{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and transforming a list of documents (in this case a list of sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can': 2, 'sun': 8, 'today': 10, 'see': 5, 'is': 4, 'bright': 1, 'we': 11, 'in': 3, 'shining': 6, 'the': 9, 'blue': 0, 'sky': 7} \n",
      "Word sky has id 7\n"
     ]
    }
   ],
   "source": [
    "v = CountVectorizer()\n",
    "\n",
    "sent = [\"The sky is blue.\", \"The sun is bright today.\", \\\n",
    "        \"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\"]\n",
    "v.fit(sent)\n",
    "\n",
    "transformed = v.transform(sent)\n",
    "\n",
    "print(v.vocabulary_, '\\nWord sky has id %d' % v.vocabulary_['sky'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the vocabulary of the fitted vectorizer and the term frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blue', 0), ('bright', 1), ('can', 2), ('in', 3), ('is', 4), ('see', 5), ('shining', 6), ('sky', 7), ('sun', 8), ('the', 9), ('today', 10), ('we', 11)]\n",
      "[[1 0 0 0 1 0 0 1 0 1 0 0]\n",
      " [0 1 0 0 1 0 0 0 1 1 1 0]\n",
      " [0 1 0 1 1 0 0 1 1 2 0 0]\n",
      " [0 1 1 0 0 1 1 0 2 2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#Getting the vocabulary dictionary and sorting it based on value (word id)\n",
    "print(sorted(v.vocabulary_.items(), key=lambda x: x[1])) \n",
    "\n",
    "print(transformed.toarray()) # The term frequency matrix for the four sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sparse matrix, a tuple of (doc_id, word_id) and the corresponding word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 9)\t2\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 8)\t2\n",
      "  (3, 9)\t2\n",
      "  (3, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming a new sentence using the same vectorizer (will maintain word ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n"
     ]
    }
   ],
   "source": [
    "sent2 = ['The moon is bright today']\n",
    "\n",
    "print(v.transform(sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The word moon doesn't appear in original vocabulary, so will not be found even after transforming new sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('moon' in v.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Tfidf Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blue', 0), ('bright', 1), ('can', 2), ('in', 3), ('is', 4), ('see', 5), ('shining', 6), ('sky', 7), ('sun', 8), ('the', 9), ('today', 10), ('we', 11)]\n",
      "['1.9163', '0.0000', '0.0000', '0.0000', '1.2231', '0.0000', '0.0000', '1.5108', '0.0000', '1.0000', '0.0000', '0.0000']\n",
      "['0.0000', '1.2231', '0.0000', '0.0000', '1.2231', '0.0000', '0.0000', '0.0000', '1.2231', '1.0000', '1.9163', '0.0000']\n",
      "['0.0000', '1.2231', '0.0000', '1.9163', '1.2231', '0.0000', '0.0000', '1.5108', '1.2231', '2.0000', '0.0000', '0.0000']\n",
      "['0.0000', '1.2231', '1.9163', '0.0000', '0.0000', '1.9163', '1.9163', '0.0000', '2.4463', '2.0000', '0.0000', '1.9163']\n"
     ]
    }
   ],
   "source": [
    "vv = TfidfVectorizer(norm = None)\n",
    "tfidf = vv.fit_transform(sent)\n",
    "print(sorted(vv.vocabulary_.items(), key=lambda x : x[1]))\n",
    "\n",
    "for row in tfidf.toarray():\n",
    "    print([\"%.4f\"% val for val in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Tfidf matrix for the first 200 documents in the simple-wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 6401)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "vv = TfidfVectorizer()\n",
    "\n",
    "docs = [] # a list of the text content of every document\n",
    "names = [] # maintains a list of file names\n",
    "for root, dirs, files in os.walk('single-docs'):\n",
    "    for file in files[:200]:\n",
    "        with open(os.path.join(root,file), 'r', encoding='utf8') as f:\n",
    "            docs.append(f.read())\n",
    "            names.append(file)\n",
    "            \n",
    "vv.fit(docs)\n",
    "\n",
    "transformed = vv.transform(docs)\n",
    "print(transformed.shape) # 200 documents by the number of unique words\n",
    "print(transformed.toarray()) # will be mostly zeros as a document contains only a small subset of the number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for cosine similarity and argmax functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989949493661\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine, cdist\n",
    "import numpy as np\n",
    "\n",
    "print(1-cosine([1,2], [1,3])) # 1 - cosine(x,y) because cosine(x,y) in scipy returns distance not similarity\n",
    "print(np.argmax([0.8,0.85,0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming a query to tf-idf (using the same vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4371)\t0.940348435107\n",
      "  (0, 3938)\t0.3402129048 \n",
      " 4371\n"
     ]
    }
   ],
   "source": [
    "q = ['president of']\n",
    "tq = vv.transform(q)\n",
    "print(tq, '\\n', vv.vocabulary_['president'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing file content having maximum similarity with query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum cosine similarity is 0.295128719449\n",
      "Document index with max similarity is 121, with file name sw_100427.txt\n",
      "\n",
      "File:\n",
      "\n",
      "Donald Tusk\n",
      "\n",
      "Donald Tusk (, born 22 April 1957) is the President of the European Council. Before that he was the Prime Minister of Poland from 2007 to 2014. He was the leader of the biggest Polish political party, Platforma Obywatelska (Civic Platform). In August 2014, Tusk was elected to become the next President of the European Council.\n",
      "\n",
      "In March 2017, Tusk was reelected as President of the European Council.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sw_100427.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generates similarities between query tf-idf matrix and document tf-idf matrix\n",
    "# Since there's only 1 query, access the first element of the array\n",
    "# Format is a list of 200 elements, where every element is the cosine similarity between the query and the ith document\n",
    "sims = 1-cdist(tq.toarray(), transformed.toarray(), metric='cosine')[0]\n",
    "\n",
    "max_sim = max(sims)\n",
    "print('Maximum cosine similarity is', max_sim)\n",
    "max_pos = np.argmax(sims)\n",
    "print('Document index with max similarity is %d, with file name %s' % (max_pos, names[max_pos]))\n",
    "\n",
    "print('\\nFile:\\n')\n",
    "\n",
    "print(open('single-docs/'+names[max_pos], 'r').read())\n",
    "print(names[max_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
